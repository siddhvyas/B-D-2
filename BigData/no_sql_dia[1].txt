!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
!tar xf spark-3.1.1-bin-hadoop3.2.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"

import findspark
findspark.init()
from pyspark.sql import SparkSession
from pyspark import SparkContext
spark = SparkSession.builder.appName('deeplearn').getOrCreate()

!wget https://raw.githubusercontent.com/neelamdoshi/Spark_neelam/main/diabetes.csv

# read a csv file
my_data = spark.read.csv('diabetes.csv',header=True)

# see the default schema of the dataframe
my_data.printSchema()

import pyspark.sql.types as tp

# define the schema
my_schema = tp.StructType([
    tp.StructField(name= 'Pregnancies', dataType= tp.IntegerType(),   nullable= True),
    tp.StructField(name= 'Glucose', dataType= tp.IntegerType(),    nullable= True),
    tp.StructField(name= 'BloodPressure',       dataType= tp.IntegerType(),   nullable= True),
    tp.StructField(name= 'SkinThickness',  dataType= tp.IntegerType(),    nullable= True),
    tp.StructField(name= 'Insulin',   dataType= tp.IntegerType(),    nullable= True),
    tp.StructField(name= 'BMI',       dataType= tp.DoubleType(),    nullable= True),
    tp.StructField(name= 'DiabetesPedigreeFunction',    dataType= tp.DoubleType(),   nullable= True),
    tp.StructField(name= 'Age',           dataType= tp.IntegerType(),   nullable= True),
    tp.StructField(name= 'Outcome',       dataType= tp.IntegerType(),   nullable= True)
])

# read the data again with the defined schema
my_data = spark.read.csv('diabetes.csv',schema= my_schema,header= True)

# print the schema
my_data.printSchema()

# get the dimensions of the data
(my_data.count() , len(my_data.columns))

my_data.head()

from pyspark.ml.feature import Imputer
imputer = Imputer(
    inputCols=my_data.columns,
    outputCols=["{}_imputed".format(c) for c in my_data.columns]
    ).setStrategy("median")


my_data1 = imputer.fit(my_data).transform(my_data)

my_data1.head(10)

from pyspark.ml.feature import VectorAssembler

# specify the input and output columns of the vector assembler
assembler = VectorAssembler(inputCols=['Pregnancies',
                                       'Glucose',
                                       'BloodPressure',
                                       'SkinThickness',
                                       'Insulin',
                                       'BMI',
                                       'DiabetesPedigreeFunction',
                                       'Age'],
                           outputCol='features')


# transform the data
final_data = assembler.transform(my_data1)

# view the transformed vector

final_data.select("features","Outcome").show(5)

from pyspark.ml.classification import LogisticRegression
xtrain, xtest = final_data.randomSplit([0.7, 0.3])

lr = LogisticRegression(featuresCol = 'features', labelCol = 'Outcome', maxIter=10)

lrModel = lr.fit(xtrain)

predictions = lrModel.transform(xtest)

predictions.show(5)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator()
evaluator.setLabelCol("Outcome")

evaluator.setPredictionCol("prediction")

evaluator.evaluate(predictions)